## ğŸ“– Sobre Este RepositÃ³rio

Este repositÃ³rio Ã© o resultado da minha jornada de aprendizado no desafio da DIO sobre **automatizaÃ§Ã£o de tarefas com AWS Lambda e S3**. Aqui documentei tudo o que aprendi, os desafios que enfrentei e os insights que obtive durante a prÃ¡tica.

---

## ğŸ“ O Que Eu Aprendi

### 1. **Conceito de Serverless Computing**

Antes deste desafio, eu tinha uma ideia vaga sobre o que era "serverless". Agora entendo que:

**Aprendi que** serverless nÃ£o significa "sem servidor", mas sim que **eu nÃ£o preciso me preocupar com a infraestrutura**. A AWS cuida de toda a parte de provisionamento, escala e manutenÃ§Ã£o dos servidores.

**O mais interessante** foi descobrir que pago apenas pelo tempo de execuÃ§Ã£o do cÃ³digo (em milissegundos!), o que torna a soluÃ§Ã£o muito econÃ´mica para automaÃ§Ãµes pontuais.

### 2. **AWS Lambda - O CoraÃ§Ã£o da AutomaÃ§Ã£o**

**Descobri que** Lambda Ã© como ter um robÃ´ que fica esperando um evento acontecer para executar uma tarefa especÃ­fica.

**Principais aprendizados:**
- Lambda executa cÃ³digo em resposta a **eventos** (triggers)
- Suporta vÃ¡rias linguagens (Python, Node.js, Java, Go, etc.)
- Tem limite de **15 minutos** de execuÃ§Ã£o por invocaÃ§Ã£o
- Posso configurar memÃ³ria de 128MB atÃ© 10GB
- **Cold start**: a primeira execuÃ§Ã£o pode ser mais lenta (aprendi isso na prÃ¡tica!)

**Insight importante**: Quanto mais memÃ³ria eu aloco, mais CPU a funÃ§Ã£o recebe proporcionalmente, o que pode compensar o custo em execuÃ§Ãµes intensivas.

### 3. **Amazon S3 - Mais do Que Armazenamento**

Eu achava que S3 era apenas um "HD na nuvem", mas **aprendi que Ã© muito mais**:

**S3 como Gatilho de Eventos:**
- Posso configurar o S3 para **notificar** outros serviÃ§os quando algo acontece
- Eventos disponÃ­veis: criaÃ§Ã£o de objeto (PUT), remoÃ§Ã£o (DELETE), restauraÃ§Ã£o, etc.
- Posso filtrar eventos por **prefixo** (pastas) ou **sufixo** (extensÃ£o de arquivo)

**Exemplo prÃ¡tico que testei:**
```
Upload de arquivo .jpg no S3
    â†“
Trigger dispara Lambda
    â†“
Lambda processa a imagem
    â†“
Resultado salvo em outro bucket
```

**Aprendi tambÃ©m sobre:**
- **Buckets**: os "containers" onde os arquivos ficam
- **Objects**: os arquivos propriamente ditos
- **Keys**: o "caminho" completo do arquivo (como `pasta/subpasta/arquivo.txt`)
- **Versionamento**: posso manter histÃ³rico de versÃµes dos arquivos

### 4. **IAM - SeguranÃ§a Ã© Fundamental**

**Este foi um dos aprendizados mais importantes**: sem as permissÃµes corretas, NADA funciona na AWS!

**Aprendi que:**
- Lambda precisa de uma **IAM Role** para acessar outros serviÃ§os
- PrincÃ­pio do **menor privilÃ©gio**: dar apenas as permissÃµes necessÃ¡rias
- TrÃªs componentes principais:
  - **PolÃ­tica (Policy)**: documento JSON que define permissÃµes
  - **Role**: "papel" que pode ser assumido por serviÃ§os
  - **Recurso**: o que estÃ¡ sendo acessado (ex: bucket especÃ­fico)

**Erro comum que cometi:**
Esqueci de dar permissÃ£o `s3:GetObject` e a Lambda nÃ£o conseguia ler os arquivos. O erro sÃ³ apareceu nos logs do CloudWatch!

### 5. **Estrutura de Eventos do S3**

**Aprendi que** quando o S3 dispara uma Lambda, ele envia um objeto JSON com todas as informaÃ§Ãµes do evento.

**Estrutura que entendi:**
```python
event = {
    'Records': [
        {
            's3': {
                'bucket': {
                    'name': 'meu-bucket'  # Nome do bucket
                },
                'object': {
                    'key': 'pasta/arquivo.txt',  # Caminho do arquivo
                    'size': 1024  # Tamanho em bytes
                }
            }
        }
    ]
}
```

**Insight**: Posso processar mÃºltiplos arquivos em uma Ãºnica invocaÃ§Ã£o, pois `Records` Ã© um array!

---

## ğŸ’¡ Insights PrÃ¡ticos

### 1. **CloudWatch Ã© Meu Melhor Amigo para Debug**

Aprendi que **SEMPRE** devo olhar os logs no CloudWatch quando algo nÃ£o funciona. Ã‰ lÃ¡ que ficam os `print()` do Python e os erros detalhados.

**Dica que descobri**: Adicionar logs descritivos facilita MUITO o troubleshooting:
```python
print(f"Processando arquivo: {file_key}")
print(f"Tamanho: {file_size} bytes")
```

### 2. **Boto3 - SDK da AWS para Python**

**Descobri que** `boto3` Ã© a biblioteca Python para interagir com serviÃ§os AWS. Ã‰ muito poderosa!

**Principais operaÃ§Ãµes que aprendi:**
```python
import boto3

# Cliente S3
s3 = boto3.client('s3')

# Ler arquivo
response = s3.get_object(Bucket='meu-bucket', Key='arquivo.txt')
conteudo = response['Body'].read()

# Escrever arquivo
s3.put_object(
    Bucket='meu-bucket',
    Key='saida/novo-arquivo.txt',
    Body='ConteÃºdo processado'
)
```

### 3. **Cold Start vs Warm Start**

**Aprendi na prÃ¡tica** a diferenÃ§a:
- **Cold Start**: primeira execuÃ§Ã£o ou apÃ³s inatividade - demora mais (pode levar segundos)
- **Warm Start**: execuÃ§Ãµes subsequentes - muito mais rÃ¡pidas (milissegundos)

**SoluÃ§Ã£o que descobri**: Para funÃ§Ãµes crÃ­ticas, posso usar "Provisioned Concurrency" para manter instÃ¢ncias aquecidas.

### 4. **Limites e Quotas**

**Descobri que** existem limites importantes:
- Tamanho do cÃ³digo: 50MB (zipado) ou 250MB (descompactado)
- Timeout mÃ¡ximo: 15 minutos
- VariÃ¡veis de ambiente: atÃ© 4KB
- Concurrent executions: 1000 por regiÃ£o (pode aumentar via suporte)

**Quando atingi o limite**: Minha primeira funÃ§Ã£o tentou processar um arquivo de 1GB e deu timeout. Aprendi a dividir o processamento em chunks!

---

## ğŸ› ï¸ Minha ImplementaÃ§Ã£o PrÃ¡tica

### Projeto 1: Conversor de Texto AutomÃ¡tico

**O que fiz**: Criei uma Lambda que converte arquivos .txt para maiÃºsculas automaticamente.

**Fluxo:**
1. Upload de arquivo `.txt` no bucket `input-bucket`
2. Lambda Ã© disparada automaticamente
3. FunÃ§Ã£o lÃª o arquivo, converte para maiÃºsculas
4. Salva resultado em `output-bucket/processed/`

**CÃ³digo que desenvolvi:**

```python
import json
import boto3
from datetime import datetime

s3 = boto3.client('s3')

def lambda_handler(event, context):
    print("ğŸš€ FunÃ§Ã£o Lambda iniciada!")
    
    for record in event['Records']:
        # Extrair informaÃ§Ãµes do evento
        bucket_origem = record['s3']['bucket']['name']
        arquivo_key = record['s3']['object']['key']
        tamanho = record['s3']['object']['size']
        
        print(f"ğŸ“„ Arquivo detectado: {arquivo_key}")
        print(f"ğŸ“¦ Bucket: {bucket_origem}")
        print(f"ğŸ’¾ Tamanho: {tamanho} bytes")
        
        try:
            # Baixar arquivo do S3
            print("â¬‡ï¸ Baixando arquivo...")
            resposta = s3.get_object(Bucket=bucket_origem, Key=arquivo_key)
            conteudo = resposta['Body'].read().decode('utf-8')
            
            print(f"âœ… Arquivo baixado com sucesso!")
            print(f"ğŸ“ ConteÃºdo original (primeiras 100 chars): {conteudo[:100]}")
            
            # Processar: converter para maiÃºsculas
            print("ğŸ”„ Convertendo para maiÃºsculas...")
            conteudo_processado = conteudo.upper()
            
            # Definir bucket e key de destino
            bucket_destino = 'meu-output-bucket'
            arquivo_saida = f"processed/{arquivo_key}"
            
            # Upload do arquivo processado
            print(f"â¬†ï¸ Fazendo upload para: {arquivo_saida}")
            s3.put_object(
                Bucket=bucket_destino,
                Key=arquivo_saida,
                Body=conteudo_processado.encode('utf-8'),
                ContentType='text/plain'
            )
            
            print("ğŸ‰ Processamento concluÃ­do!")
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'mensagem': 'Sucesso!',
                    'arquivo_original': arquivo_key,
                    'arquivo_processado': arquivo_saida,
                    'timestamp': datetime.now().isoformat()
                })
            }
            
        except Exception as erro:
            print(f"âŒ ERRO: {str(erro)}")
            return {
                'statusCode': 500,
                'body': json.dumps({
                    'erro': str(erro)
                })
            }
```

**O que aprendi desenvolvendo isso:**
- Como extrair informaÃ§Ãµes do evento S3
- A importÃ¢ncia de logs descritivos (os emojis ajudam a visualizar!)
- Tratamento de erros Ã© ESSENCIAL
- Sempre converter encoding (`.decode()` e `.encode()`)

---

## ğŸ› Problemas Que Enfrentei e Como Resolvi

### Problema 1: "Access Denied" ao Ler do S3

**O que aconteceu**: Minha Lambda nÃ£o conseguia ler arquivos do S3.

**Erro nos logs**:
```
An error occurred (AccessDenied) when calling the GetObject operation
```

**Como resolvi**:
1. Fui no IAM e verifiquei a Role da Lambda
2. Adicionei a permissÃ£o `s3:GetObject` para o bucket especÃ­fico
3. **LiÃ§Ã£o aprendida**: SEMPRE verificar as permissÃµes primeiro!

### Problema 2: FunÃ§Ã£o Dando Timeout

**O que aconteceu**: Lambda parava de executar apÃ³s 3 segundos (timeout padrÃ£o).

**Como resolvi**:
1. Aumentei o timeout nas configuraÃ§Ãµes da Lambda para 30 segundos
2. **LiÃ§Ã£o aprendida**: Para processamento de arquivos maiores, preciso ajustar o timeout!

### Problema 3: Erro "Module Not Found"

**O que aconteceu**: Tentei usar a biblioteca `Pillow` mas recebi erro de mÃ³dulo nÃ£o encontrado.

**Como resolvi**:
1. Aprendi sobre **Lambda Layers** - onde posso adicionar dependÃªncias
2. Criei um layer com Pillow e anexei Ã  minha funÃ§Ã£o
3. **Alternativa**: usar uma imagem Docker customizada
4. **LiÃ§Ã£o aprendida**: Lambda nÃ£o vem com todas as bibliotecas Python por padrÃ£o!

### Problema 4: Trigger Sendo Disparado em Loop

**O que aconteceu**: Minha Lambda salvava no mesmo bucket de entrada, causando trigger infinito! ğŸ˜±

**Como resolvi**:
1. Criei um bucket separado para saÃ­da (`output-bucket`)
2. Configurei o trigger apenas no bucket de entrada
3. **LiÃ§Ã£o aprendida**: NUNCA processar e salvar no mesmo bucket que dispara o trigger!

---

## ğŸ’° O Que Aprendi Sobre Custos

### Modelo de CobranÃ§a do Lambda

**Free Tier (sempre grÃ¡tis):**
- 1 milhÃ£o de requests por mÃªs
- 400.000 GB-segundos de compute time por mÃªs

**Depois do Free Tier:**
- $0.20 por 1 milhÃ£o de requests
- $0.0000166667 por GB-segundo

**Exemplo de cÃ¡lculo que fiz:**
```
CenÃ¡rio: 10.000 execuÃ§Ãµes/mÃªs, 512MB RAM, 2 segundos cada

Compute: 10.000 Ã— 0.5GB Ã— 2s = 10.000 GB-segundos
Custo Compute: (10.000 - 400.000 free tier) = $0 (dentro do free tier!)

Requests: 10.000 requests
Custo Requests: (10.000 - 1.000.000 free tier) = $0 (dentro do free tier!)

Total: GRÃTIS! ğŸ‰
```

**Insight importante**: Para a maioria dos casos de automaÃ§Ã£o simples, fica no free tier!

### Custos do S3

**Storage:**
- $0.023 per GB/mÃªs (Standard)
- $0.0125 per GB/mÃªs (Infrequent Access)

**Requests:**
- PUT/POST: $0.005 per 1.000 requests
- GET: $0.0004 per 1.000 requests

**Aprendi que**: Para arquivos acessados raramente, usar S3 Glacier pode economizar muito!

---

## ğŸ¯ Casos de Uso Reais Que Pensei

ApÃ³s aprender tudo isso, identifiquei cenÃ¡rios reais onde posso aplicar:

### 1. **Processamento de Notas Fiscais**
- Upload de XML â†’ Lambda extrai dados â†’ Salva em banco de dados
- **Economia**: NÃ£o preciso manter servidor rodando 24/7

### 2. **Backup AutomÃ¡tico de Logs**
- App gera log diÃ¡rio â†’ Lambda compacta â†’ Move para S3 Glacier
- **BenefÃ­cio**: ReduÃ§Ã£o de 75% no custo de armazenamento

### 3. **GeraÃ§Ã£o de RelatÃ³rios**
- Upload de CSV â†’ Lambda processa â†’ Gera PDF â†’ Envia por email
- **Vantagem**: Processamento sob demanda

### 4. **OtimizaÃ§Ã£o de Imagens para Web**
- Upload de foto â†’ Lambda redimensiona â†’ Cria mÃºltiplos tamanhos
- **AplicaÃ§Ã£o**: Site com carregamento mais rÃ¡pido

### 5. **ValidaÃ§Ã£o de Arquivos CSV**
- Upload â†’ Lambda valida formato â†’ Move para pasta correta ou rejeita
- **Uso**: Pipeline de dados automatizado

---

## ğŸ“Š Monitoramento e Observabilidade

### CloudWatch - Meu Painel de Controle

**Aprendi a usar:**

1. **CloudWatch Logs**: Onde ficam os `print()` do cÃ³digo
   - Cada execuÃ§Ã£o cria um "log stream"
   - Posso filtrar logs por palavra-chave
   - RetenÃ§Ã£o configurÃ¡vel (1 dia atÃ© âˆ)

2. **CloudWatch Metrics**: MÃ©tricas automÃ¡ticas
   - Invocations (nÃºmero de execuÃ§Ãµes)
   - Duration (tempo de execuÃ§Ã£o)
   - Errors (erros ocorridos)
   - Throttles (execuÃ§Ãµes limitadas)

3. **CloudWatch Alarms**: Alertas automÃ¡ticos
   - Posso criar alarmes se erros > 5 em 5 minutos
   - Integra com SNS para enviar email/SMS

**PrÃ¡tica que adotei:**
```python
import logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    logger.info(f"Evento recebido: {event}")
    # ... resto do cÃ³digo
```

---

## ğŸš€ PrÃ³ximos Passos no Meu Aprendizado

Agora que domino o bÃ¡sico, quero explorar:

âœ… **Lambda Layers**: Para reutilizar cÃ³digo entre funÃ§Ãµes  
âœ… **Step Functions**: Orquestrar mÃºltiplas Lambdas  
âœ… **DynamoDB + Lambda**: Construir APIs serverless  
âœ… **Lambda@Edge**: Processar requests no CloudFront  
âœ… **EventBridge**: Triggers mais avanÃ§ados  
âœ… **SAM (Serverless Application Model)**: Infraestrutura como cÃ³digo  

---

## ğŸ“š Recursos Que Me Ajudaram

### DocumentaÃ§Ã£o Oficial
- [AWS Lambda Documentation](https://docs.aws.amazon.com/lambda/)
- [Amazon S3 Documentation](https://docs.aws.amazon.com/s3/)
- [Boto3 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)

### Tutoriais que Segui
- [AWS Lambda Tutorial](https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html)
- [S3 Event Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html)

### Comunidade
- [AWS re:Post](https://repost.aws/) - Forum oficial da AWS
- [Stack Overflow - aws-lambda tag](https://stackoverflow.com/questions/tagged/aws-lambda)

---

## ğŸ† ConclusÃ£o e ReflexÃµes

Este desafio foi **muito alÃ©m de apenas aprender ferramentas tÃ©cnicas**. Eu desenvolvi:

**Habilidades TÃ©cnicas:**
- âœ… Arquitetura serverless
- âœ… AutomaÃ§Ã£o de processos
- âœ… Cloud computing (AWS)
- âœ… Gerenciamento de permissÃµes (IAM)
- âœ… Debugging e troubleshooting

**Habilidades Comportamentais:**
- âœ… ResoluÃ§Ã£o de problemas
- âœ… PersistÃªncia (muitos erros no caminho!)
- âœ… DocumentaÃ§Ã£o tÃ©cnica
- âœ… Aprendizado autodidata

**Principais Takeaways:**

1. **Serverless Ã© poderoso**: Posso criar automaÃ§Ãµes robustas sem gerenciar servidores
2. **SeguranÃ§a Ã© fundamental**: IAM pode ser complicado, mas Ã© essencial entender
3. **Logs salvam vidas**: CloudWatch Ã© imprescindÃ­vel para debug
4. **ComeÃ§ar simples**: Comecei com um "Hello World", depois evolui para casos reais
5. **Documentar Ã© essencial**: Este README me ajudarÃ¡ no futuro!

**O que mais me surpreendeu:**
A facilidade de integraÃ§Ã£o entre os serviÃ§os AWS. Literalmente alguns cliques e meu S3 jÃ¡ estava conversando com a Lambda!

**Desafio que pretendo fazer agora:**
Construir um sistema completo de processamento de imagens com mÃºltiplas Lambdas orquestradas!


## ğŸ¤ ContribuiÃ§Ãµes

Este Ã© um repositÃ³rio de aprendizado pessoal, mas feedbacks sÃ£o sempre bem-vindos!

Se vocÃª tambÃ©m estÃ¡ aprendendo AWS, sinta-se Ã  vontade para:
- â­ Dar uma estrela no repositÃ³rio
- ğŸ› Reportar erros ou sugestÃµes
- ğŸ’¬ Compartilhar suas prÃ³prias experiÃªncias
